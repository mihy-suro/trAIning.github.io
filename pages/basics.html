<!-- BASICS -->
<section id="basics">
  <div class="section section--muted">
    <h2 class="section__title">Jak jazykovÃ© modely skuteÄnÄ› fungujÃ­</h2>
     <div class="section__body">

      <!-- Intro -->

        <p>
          VeÅ™ejnÃ¡ debata o jazykovÃ½ch modelech (LLM) Äasto sklouzÃ¡vÃ¡ ke dvÄ›ma krajnÃ­m polohÃ¡m. Podle jednÄ›ch jsou v zÃ¡sadÄ› k niÄemu a jde jen o nÄ›co chytÅ™ejÅ¡Ã­ naÅ¡eptÃ¡vaÄ z Nokie 3310. Podle druhÃ½ch jde naopak o technologii, kterÃ¡ nÃ¡s pÅ™ipravÃ­ o prÃ¡ci, nebo nÃ¡s rovnou vyhubÃ­ ve stylu Skynetu z TerminÃ¡tora.
        </p>
        <p>
          Pravda je, Å¾e tak trochu platÃ­ obojÃ­ ğŸ™‚, ale stejnÄ› jako u vÄ›tÅ¡iny vÄ›cÃ­ kolem LLM velmi zÃ¡leÅ¾Ã­ na kontextu a na tom, co od nich oÄekÃ¡vÃ¡me.
        </p>
        <p>
          NejlepÅ¡Ã­ cestou, jak si v tom udÄ›lat jasno, je struÄnÄ› si vysvÄ›tlit, jak jazykovÃ© modely zhruba fungujÃ­. DÃ­ky tomu zÃ­skÃ¡me realistiÄtÄ›jÅ¡Ã­ pÅ™edstavu o tom, co od nich lze Äekat â€“ a kde naopak narÃ¡Å¾ejÃ­ na svÃ© limity. V nÃ¡sledujÃ­cÃ­ ÄÃ¡sti se proto podÃ­vÃ¡me na nÄ›kolik zÃ¡kladnÃ­ch principÅ¯: jak se tyto modely uÄÃ­, jak reprezentujÃ­ text a jakÃ½m zpÅ¯sobem generujÃ­ odpovÄ›di.
        </p>

        <p><a href="#AIIntro" class="link--large">â‡£ AI, ML, LLMâ€¦ aneb co je vlastnÄ› co</a></p>
        <p><a href="#NeuronovaSit" class="link--large">â‡£ Co je neuronovÃ¡ sÃ­Å¥</a></p>
        <p><a href="#Transformery" class="link--large">â‡£ Transformery a Attention</a></p>
        <p><a href="#JakSeUci" class="link--large">â‡£ Jak se LLMs uÄÃ­</a></p>
        <p><a href="#ZakladniTrenink" class="link--large">â‡£ ZÃ¡kladnÃ­ jazykovÃ½ trÃ©nink</a></p>
        <p><a href="#Reinforcement" class="link--large">â‡£ Reinforcement learning</a></p>
        <p><a href="#JakChapeText" class="link--large">â‡£ Jak LLM "chÃ¡pe" text</a></p>

        <section id="AIIntro">  </section>
        <h1>AI, ML, LLMâ€¦ aneb co je vlastnÄ› co</h1>
        <p>
          Pojem AI se dnes naduÅ¾Ã­vÃ¡ natolik, Å¾e obÄas pÅ™ipomÃ­nÃ¡ spÃ­Å¡ zaklÃ­nadlo, kterÃ© mÃ¡ pomoci zÃ­skat grant nebo pÅ¯sobit, Å¾e â€jedeme trendy". PrÃ¡vÄ› proto mÃ¡ smysl si hned na zaÄÃ¡tku alespoÅˆ rÃ¡mcovÄ› ujasnit, co se pod tÃ­mto pojmem skuteÄnÄ› myslÃ­.
        </p>
        <p>
          UmÄ›lÃ¡ inteligence je zastÅ™eÅ¡ujÃ­cÃ­ pojem pro systÃ©my, kterÃ© se chovajÃ­ â€chytÅ™e" â€“ od jednoduchÃ½ch pravidlovÃ½ch Å™eÅ¡enÃ­ (napÅ™. filtrovÃ¡nÃ­ e-mailÅ¯) aÅ¾ po pokroÄilÃ© systÃ©my pracujÃ­cÃ­ s daty. Jednou z oblastÃ­ AI je strojovÃ© uÄenÃ­ kde se systÃ©my uÄÃ­ z dat a tÃ­m zlepÅ¡ujÃ­ svÃ© chovÃ¡nÃ­. JednÃ­m z dÅ¯leÅ¾itÃ½ch pÅ™Ã­stupÅ¯ ve strojovÃ©m uÄenÃ­ jsou neuronovÃ© sÃ­tÄ›, velmi volnÄ› inspirovanÃ© fungovÃ¡nÃ­m lidskÃ©ho mozku. Ty se dnes pouÅ¾Ã­vajÃ­ u Å™ady Ãºloh, kde je potÅ™eba rozpoznÃ¡vat vzory v datech â€“ napÅ™Ã­klad v obrazu, zvuku nebo textu.
        </p>
        <p>
          GenerativnÃ­ modely pÅ™edstavujÃ­ specializovanou skupinu modelÅ¯, Äasto zaloÅ¾enÃ½ch prÃ¡vÄ› na neuronovÃ½ch sÃ­tÃ­ch. DokÃ¡Å¾ou vytvÃ¡Å™et novÃ½ obsah, jako je text, obrÃ¡zky, zvuk nebo kÃ³d. Mezi nÄ› patÅ™Ã­ i velkÃ© jazykovÃ© modely (LLM), kterÃ© se zamÄ›Å™ujÃ­ na prÃ¡ci s pÅ™irozenÃ½m jazykem a kam patÅ™Ã­ nÃ¡stroje typu ChatGPT.
        </p>
        <p>
          V bÄ›Å¾nÃ© Å™eÄi se tyto pojmy Äasto zamÄ›ÅˆujÃ­. Nechceme slovÃ­ÄkaÅ™it, ale pÅ™ece jen je uÅ¾iteÄnÃ© vÄ›dÄ›t, jak spolu jednotlivÃ© pojmy souvisejÃ­.
        </p>

        <!-- Obr. AI / ML / LLM -->
        <figure class="image-card">
          <div class="image-card__frame">
            <img
                src="img/venn_diagram.png"
                alt="VennÅ¯v diagram znÃ¡zorÅˆujÃ­cÃ­ pÅ™ekryv mezi umÄ›lou inteligencÃ­, strojovÃ½m uÄenÃ­m a generativnÃ­mi modely."
                loading="lazy"
                decoding="async"
            />
          </div>
        </figure>

        <!-- Co je neuronovÃ¡ sÃ­Å¥ -->
        <section id="NeuronovaSit"> </section> 
        <h1>NeuronovÃ© sÃ­tÄ›, jazykovÃ© modely a proÄ to celÃ© funguje</h1>
        <p>
          NeuronovÃ¡ sÃ­Å¥ je matematickÃ½ model, kterÃ½ je volnÄ› inspirovanÃ½ fungovÃ¡nÃ­m neuronÅ¯ v lidskÃ©m mozku. DÃ¡ se na ni dÃ­vat jako na jakÃ½si â€matematickÃ½ mozek", sloÅ¾enÃ½ z velkÃ©ho mnoÅ¾stvÃ­ propojenÃ½ch uzlÅ¯ (neuronÅ¯), kterÃ© spoleÄnÄ› zpracovÃ¡vajÃ­ informace. NeuronovÃ¡ sÃ­Å¥ je popsÃ¡na pomocÃ­ velkÃ©ho mnoÅ¾stvÃ­ parametrÅ¯ (tzv. vah). Ty urÄujÃ­, jak silnÄ› spolu jednotlivÃ© ÄÃ¡sti sÃ­tÄ› komunikujÃ­. Tyto parametry nejsou pevnÄ› danÃ© a bÄ›hem trÃ©novÃ¡nÃ­ se postupnÄ› upravujÃ­ na zÃ¡kladÄ› dat, aby sÃ­Å¥ dÃ¡vala lepÅ¡Ã­ vÃ½sledky. Pokud mÃ¡ sÃ­Å¥ mnoho vrstev nad sebou, mluvÃ­me o hlubokÃ© neuronovÃ© sÃ­ti.
        </p>
        <p>
          UÄenÃ­ funguje zjednoduÅ¡enÄ› tak, Å¾e sÃ­Å¥ dostane vstup (napÅ™Ã­klad text), vytvoÅ™Ã­ vÃ½stup a ten se porovnÃ¡ s tÃ­m, co bychom chtÄ›li, aby sprÃ¡vnÄ› vzniklo. Pokud se sÃ­Å¥ splete, jejÃ­ parametry se lehce upravÃ­. Tento proces se opakuje znovu a znovu nad velkÃ½m mnoÅ¾stvÃ­m dat. Mechanismus, kterÃ½ tyto Ãºpravy Å™Ã­dÃ­, se nazÃ½vÃ¡ zpÄ›tnÃ© Å¡Ã­Å™enÃ­ chyby (backpropagation) a Äasto se kombinuje s optimalizaÄnÃ­mi algoritmy typu SGD. PrÃ¡vÄ› takto trÃ©novanÃ© hlubokÃ© neuronovÃ© sÃ­tÄ› dnes tvoÅ™Ã­ zÃ¡klad velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM).
        </p>
        <p>
          DÃ­ky obrovskÃ©mu mnoÅ¾stvÃ­ parametrÅ¯ a dat se tyto modely dokÃ¡Å¾ou nauÄit sloÅ¾itÃ© jazykovÃ© vzory â€“ napÅ™Ã­klad gramatiku, vÃ½znam slov nebo kontext v delÅ¡Ã­ch textech. NeznamenÃ¡ to ale, Å¾e â€rozumÃ­" textu jako ÄlovÄ›k; spÃ­Å¡e velmi dobÅ™e odhadujÃ­, co pravdÄ›podobnÄ› nÃ¡sleduje.
        </p>

        <!-- PoznÃ¡mka: VÄ›tÅ¡Ã­ model = lepÅ¡Ã­? -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-vetsi-model" id="collapsible-button-vetsi-model">
          <b>VÃCE:</b> Je vÄ›tÅ¡Ã­ model automaticky lepÅ¡Ã­?
          </button>

          <div class="collapsible__panel" id="collapsible-panel-vetsi-model" role="region" aria-labelledby="collapsible-button-vetsi-model" hidden>
            <p>
              Ne nutnÄ›. VÅ¾dy zÃ¡leÅ¾Ã­ na konkrÃ©tnÃ­m Ãºkolu. MenÅ¡Ã­, specializovanÃ½ model mÅ¯Å¾e v ÃºzkÃ© oblasti podat lepÅ¡Ã­ vÃ½sledky neÅ¾ nejnovÄ›jÅ¡Ã­ monstrum od OpenAI. NevÃ½hodou je, Å¾e obvykle hÅ¯Å™e zobecÅˆuje a mimo svou specializaci rychle narÃ¡Å¾Ã­ na limity. Na druhou stranu je lze Äasto provozovat lokÃ¡lnÄ› na vlastnÃ­m poÄÃ­taÄi a jejich odezva mÅ¯Å¾e bÃ½t vÃ½raznÄ› rychlejÅ¡Ã­ neÅ¾ u velkÃ½ch modelÅ¯ bÄ›Å¾Ã­cÃ­ch v cloudu. Je to logickÃ© â€“ menÅ¡Ã­ model potÅ™ebuje pro predikci niÅ¾Å¡Ã­ poÄet operacÃ­ (hlavnÄ› maticovÃ©ho nÃ¡sobenÃ­). ObecnÄ› sice platÃ­, Å¾e vÄ›tÅ¡Ã­ poÄet parametrÅ¯ Äasto znamenÃ¡ vyÅ¡Å¡Ã­ schopnosti (napÅ™. podle tzv. Chinchilla scaling laws), ale nejde o univerzÃ¡lnÃ­ pravidlo.
            </p>
          </div>
        </div>

        <!-- PoznÃ¡mka: Chinchilla scaling laws -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-chinchilla" id="collapsible-button-chinchilla">
          <b>VÃCE:</b> VÅ¡echno Å™Ã­dÃ­ ÄinÄily!
          </button>

          <div class="collapsible__panel" id="collapsible-panel-chinchilla" role="region" aria-labelledby="collapsible-button-chinchilla" hidden>
            <p>
              <strong>Chinchilla scaling laws</strong> popisujÃ­ optimÃ¡lnÃ­ vztah mezi velikostÃ­ modelu (poÄet parametrÅ¯) a mnoÅ¾stvÃ­m trÃ©novacÃ­ch dat (poÄet tokenÅ¯) pro danÃ½ vÃ½poÄetnÃ­ rozpoÄet. Studie DeepMind ukÃ¡zala, Å¾e pro dosaÅ¾enÃ­ co nejlepÅ¡Ã­ho vÃ½konu by poÄet trÃ©novacÃ­ch tokenÅ¯ mÄ›l bÃ½t asi 20Ã— vÄ›tÅ¡Ã­ neÅ¾ poÄet parametrÅ¯ modelu â€“ tedy model s 1 mld. parametrÅ¯ by mÄ›l bÃ½t trÃ©novÃ¡n na zhruba 20 mld. tokenÅ¯.
            </p>
            <p>
              V matematickÃ© formÄ› se Chinchilla scaling laws Äasto vyjadÅ™ujÃ­ skrze souvislost mezi vÃ½poÄetnÃ­mi nÃ¡klady $C$, poÄtem parametrÅ¯ $N$ a poÄtem trÃ©novacÃ­ch tokenÅ¯ $D$:
            </p>
            <p style="text-align: center; font-size: 1.2em;">
              $$C \approx C_0 \cdot N \cdot D$$
            </p>
            <p>
              kde $C_0$ je konstanta pÅ™evÃ¡dÄ›jÃ­cÃ­ poÄet operacÃ­ na nÃ¡klady. Po optimalizaci se zjistÃ­, Å¾e optimÃ¡lnÃ­ kombinace $N$ a $D$ minimalizuje chybovou funkci modelu pro danÃ½ rozpoÄet.
            </p>
            <p>
              Chinchilla tÃ­m zpochybnila dÅ™Ã­vÄ›jÅ¡Ã­ pÅ™edstavu, Å¾e vÄ›tÅ¡Ã­ model = lepÅ¡Ã­ model, a mÃ­sto toho doporuÄuje vyvÃ¡Å¾enÃ½ pomÄ›r parametrÅ¯ a dat pro efektivnÄ›jÅ¡Ã­ uÄenÃ­.
            </p>
          </div>
        </div>

        <!-- PoznÃ¡mka: Overfitting a double descent -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-overfitting" id="collapsible-button-overfitting">
          <b>VÃCE:</b> A nejde to celÃ© proti zÃ¡kladnÃ­m principÅ¯m modelovÃ¡nÃ­?
          </button>

          <div class="collapsible__panel" id="collapsible-panel-overfitting" role="region" aria-labelledby="collapsible-button-overfitting" hidden>
            <p>
              KdyÅ¾ se Å™ekne, Å¾e dneÅ¡nÃ­ hlubokÃ© neuronovÃ© sÃ­tÄ› mohou mÃ­t stovky miliard parametrÅ¯, nÄ›kterÃ½m z vÃ¡s se moÅ¾nÃ¡ na chvÃ­li udÄ›lalo lehce nevolno. KaÅ¾dÃ½, kdo si nÄ›kdy v Excelu zkusil proloÅ¾it pÄ›t datovÃ½ch bodÅ¯ polynomem pÃ¡tÃ©ho stupnÄ›, teÄ automaticky zvedÃ¡ oboÄÃ­ â€“ a to zcela oprÃ¡vnÄ›nÄ›. S tolika parametry totiÅ¾ hrozÃ­ pÅ™euÄenÃ­ (overfitting), tedy situace, kdy se model mÃ­sto obecnÃ©ho porozumÄ›nÃ­ nauÄÃ­ trÃ©novacÃ­ data zpamÄ›ti. PÅ™i trÃ©novÃ¡nÃ­ je pak nutnÃ© proces vÄas zastavit, jinak se schopnost modelu zobecÅˆovat zaÄne zhorÅ¡ovat. CÃ­lem totiÅ¾ nenÃ­ vychovat â€tÅ™Ã­dnÃ­ho Å¡prta", kterÃ½ se nauÄÃ­ uÄebnici slovo od slova, ale spÃ­Å¡e trochu lajdÃ¡ckÃ©ho gÃ©nia, kterÃ½ si dokÃ¡Å¾e poradit i s Ãºlohami, kterÃ© nikdy pÅ™edtÃ­m nevidÄ›l.
            </p>
            <p>
              ZajÃ­mavÃ© (a pro intuici trochu zneklidÅˆujÃ­cÃ­) je, Å¾e u velmi velkÃ½ch hlubokÃ½ch sÃ­tÃ­ se Äasto pozoruje opaÄnÃ½ efekt. KdyÅ¾ model dÃ¡l zvÄ›tÅ¡ujeme a pÅ™idÃ¡vÃ¡me dalÅ¡Ã­ vrstvy a parametry, v urÄitÃ©m bodÄ› se jeho schopnost zobecÅˆovat zaÄne znovu zlepÅ¡ovat. Tento jev se oznaÄuje jako double descent a dodnes nenÃ­ ÃºplnÄ› vysvÄ›tlen.
            </p>
            <p>
              Jednou z moÅ¾nÃ½ch interpretacÃ­ je tzv. hypotÃ©za vÃ½hernÃ­ho losu (lottery ticket hypothesis). Ta Å™Ã­kÃ¡, Å¾e v obÅ™Ã­ neuronovÃ© sÃ­ti se bÄ›hem trÃ©novÃ¡nÃ­ tak trochu â€nÃ¡hodou" najde menÅ¡Ã­ podstruktura parametrÅ¯, kterÃ¡ je pro danÃ½ Ãºkol mimoÅ™Ã¡dnÄ› dobÅ™e pÅ™izpÅ¯sobenÃ¡ â€“ a zbytek sÃ­tÄ› jÃ­ v podstatÄ› jen dÄ›lÃ¡ statistickÃ½ doprovod.
            </p>
          </div>
        </div>

      <!-- Int. Obr: Schema neuronovÃ© sÃ­tÄ› -->
        <figure class="image-card">
          <div class="image-card__frame">
          <div class="hotspot-image">
            <img src="img/neuron_network.png" alt="SchÃ©ma neuronovÃ© sÃ­tÄ›" loading="lazy" decoding="async"/>
            
            <!-- Hotspot -->
            <div class="hotspot" style="left: 24%; top: 10%;">
              <div class="hotspot__tooltip hotspot__tooltip--below" role="tooltip" hidden>
                <strong>Data, ze kterÃ½ch se model uÄÃ­</strong><br>
                ÄŒÃ­selnÃ© reprezentace textu, obrazu nebo jinÃ½ch vstupÅ¯, kterÃ© sÃ­Å¥ pÅ™ijÃ­mÃ¡ jako zÃ¡klad pro dalÅ¡Ã­ zpracovÃ¡nÃ­.
              </div>
            </div>
            
            <!-- Hotspot -->
            <div class="hotspot" style="left: 47%; top: 10%;">
              <div class="hotspot__tooltip hotspot__tooltip--below" role="tooltip" hidden>
                <strong>VÃ½poÄetnÃ­ vrstvy</strong><br>
                V tÃ©to ÄÃ¡sti sÃ­Å¥ krok za krokem kombinuje vstupnÃ­ data pomocÃ­ uzlÅ¯ a vah. PostupnÄ› se uÄÃ­ rozpoznÃ¡vat 
                dÅ¯leÅ¾itÃ© rysy a vztahy v datech â€“ od jednoduchÃ½ch vzorcÅ¯ aÅ¾ po sloÅ¾itÄ›jÅ¡Ã­ souvislosti, kterÃ© nejsou 
                na prvnÃ­ pohled patrnÃ©.
              </div>
            </div>
            
            <!-- Hotspot -->
            <div class="hotspot" style="left: 91%; top: 80%;">
              <div class="hotspot__tooltip" role="tooltip" hidden>
                <strong>VÃ½stup modelu</strong><br>
                PÅ™i trÃ©novÃ¡nÃ­ se vÃ½stupy prÅ¯bÄ›Å¾nÄ› porovnÃ¡vajÃ­ s referenÄnÃ­mi trÃ©novacÃ­mi daty. 
                Na zÃ¡kladÄ› rozdÃ­lu mezi pÅ™edpovÄ›dÃ­ a skuteÄnÃ½mi hodnotami se upravujÃ­ vÃ¡hy tak, aby model postupnÄ› 
                dokÃ¡zal samostatnÄ› predikovat sprÃ¡vnÃ½ vÃ½stup.
              </div>
            </div>
          </div>
          </div>
          <figcaption class="image-card__caption">
            <h3 class="image-card__title">SchematickÃ© znÃ¡zornÄ›nÃ­ neuronovÃ© sÃ­tÄ› urÄenÃ© ke klasifikaci fotografiÃ­ psÅ¯ a koÄek.</h3>
            <p class="image-card__meta">
              
              NeuronovÃ¡ sÃ­Å¥ je tvoÅ™ena nÄ›kolika vrstvami propojenÃ½ch â€neuronÅ¯" (uzlÅ¯). JednotlivÃ© uzly jsou spojeny vahami, kterÃ© urÄujÃ­ sÃ­lu vzÃ¡jemnÃ©ho propojenÃ­. VstupnÃ­ data (napÅ™. hodnoty pixelÅ¯ obrÃ¡zku) postupnÄ› prochÃ¡zejÃ­ sÃ­tÃ­ vrstvu po vrstvÄ›. KaÅ¾dÃ½ neuron kombinuje svÃ© vstupy pomocÃ­ vah a nelineÃ¡rnÃ­

              <span class="info" data-info>
                <span class="info__trigger info__word" tabindex="0" role="button" aria-label="VÃ­ce informacÃ­ o aktivaÄnÃ­ funkci">
                  aktivaÄnÃ­ funkce
                </span>
                <span class="info__tooltip" role="tooltip" hidden>
                  AktivaÄnÃ­ funkce zavÃ¡dÃ­ nelinearitu do modelu, coÅ¾ umoÅ¾Åˆuje sÃ­ti zachytit sloÅ¾itÄ›jÅ¡Ã­ vzory v datech. Mezi bÄ›Å¾nÃ© aktivaÄnÃ­ funkce patÅ™Ã­ ReLU (Rectified Linear Unit), sigmoidnÃ­ funkce a tanh.
                </span>
              </span> a vytvÃ¡Å™Ã­ tak dÃ­lÄÃ­ vÃ½stup.
              BÄ›hem trÃ©novÃ¡nÃ­ se vÃ¡hy tÄ›chto spojenÃ­ automaticky upravujÃ­ tak, aby sÃ­Å¥ co nejlÃ©pe rozliÅ¡ila sprÃ¡vnou kategorii â€“ v tomto pÅ™Ã­padÄ› zda obrÃ¡zek obsahuje psa, nebo koÄku.

            </p>
          </figcaption>
        </figure>

      <p>
        JeÅ¡tÄ› pÅ™ed nÃ¡stupem velkÃ½ch jazykovÃ½ch modelÅ¯ jste se moÅ¾nÃ¡ setkali s chatboty na e-shopech, kterÃ© slibujÃ­ rychlou pomoc, ale Äasto spÃ­Å¡ obtÄ›Å¾ujÃ­. I tyto systÃ©my lze povaÅ¾ovat za formu umÄ›lÃ© inteligence, jsou vÅ¡ak velmi jednoduchÃ© a tematicky omezenÃ©. Typicky fungujÃ­ tak, Å¾e v textu hledajÃ­ klÃ­ÄovÃ¡ slova a na jejich zÃ¡kladÄ› vybÃ­rajÃ­ pÅ™edem pÅ™ipravenÃ© odpovÄ›di â€“ bez skuteÄnÃ© prÃ¡ce s kontextem.
      </p>
      <p>
        ZÃ¡sadnÃ­ prÅ¯lom u LLM pÅ™inesla architektura <b>transformer</b>, kterÃ¡ umoÅ¾Åˆuje modelu pracovat s rÅ¯znÃ½mi ÄÃ¡stmi textu najednou a lÃ©pe zachytit kontext, vÄetnÄ› vzdÃ¡lenÃ½ch souvislostÃ­ v textu. PrÃ¡vÄ› schopnost prÃ¡ce s kontextem je jednÃ­m z hlavnÃ­ch dÅ¯vodÅ¯, proÄ dneÅ¡nÃ­ jazykovÃ© modely pÅ¯sobÃ­ vÃ½raznÄ› â€chytÅ™eji" neÅ¾ starÅ¡Ã­ pÅ™Ã­stupy.
      </p>
      <p>
        Mimochodem, slovo <i>transformer</i> stojÃ­ i za pÃ­smenem â€T" ve zkratce <b>GPT</b>. GPT znamenÃ¡ Generative Pre-trained Transformer:
      </p>
      <ul>
        <li><b>generative</b> â€“ model vytvÃ¡Å™Ã­ novÃ½ obsah,</li>
        <li><b>pre-trained</b> â€“ je pÅ™edtrÃ©novÃ¡n na rozsÃ¡hlÃ½ch datech,</li>
        <li><b>transformer</b> â€“ vyuÅ¾Ã­vÃ¡ architekturu zaloÅ¾enou na mechanismu attention.</li>
      </ul>
      <p>
        GPT tedy neoznaÄuje jeden konkrÃ©tnÃ­ model (napÅ™Ã­klad ChatGPT), ale celou rodinu pÅ™Ã­buznÃ½ch modelÅ¯, kterÃ© se liÅ¡Ã­ velikostÃ­, schopnostmi i oblastmi pouÅ¾itÃ­. VÅ¡echny spadajÃ­ do Å¡irÅ¡Ã­ kategorie velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM).
      </p>

        <section id="Transformery"> </section>
        <!-- Transformery a Attention -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-basics-1" id="collapsible-button-basics-1">
          <b>VÃCE:</b> Transformery a attention
          </button>

          <div class="collapsible__panel" id="collapsible-panel-basics-1" role="region" aria-labelledby="collapsible-button-basics-1" hidden>
            <p>
              Transformery jsou typ neuronovÃ½ch sÃ­tÃ­, kterÃ© tvoÅ™Ã­ zÃ¡klad vÄ›tÅ¡iny souÄasnÃ½ch generativnÃ­ch modelÅ¯, zejmÃ©na jazykovÃ½ch modelÅ¯. Na rozdÃ­l od starÅ¡Ã­ch pÅ™Ã­stupÅ¯ nezpracovÃ¡vajÃ­ data postupnÄ› krok za krokem, ale pracujÃ­ s celÃ½m vstupem najednou. DÃ­ky tomu dokÃ¡Å¾ou efektivnÄ› zachytit Å¡irÅ¡Ã­ kontext a lÃ©pe se uplatÅˆujÃ­ pÅ™i prÃ¡ci s dlouhÃ½mi texty, obrazy nebo obecnÄ› komplexnÃ­mi daty.
            </p>
            <p>
              KlÃ­ÄovÃ½m prvkem transformerÅ¯ je mechanismus <b>attention</b> (pozornosti). Ten umoÅ¾Åˆuje modelu rozhodovat, kterÃ© ÄÃ¡sti vstupu jsou v danÃ©m okamÅ¾iku dÅ¯leÅ¾itÃ© a jak spolu jednotlivÃ© ÄÃ¡sti souvisejÃ­. NapÅ™Ã­klad pÅ™i zpracovÃ¡nÃ­ vÄ›ty si model dokÃ¡Å¾e â€vÅ¡imnout", ke kterÃ©mu podstatnÃ©mu jmÃ©nu se vztahuje zÃ¡jmeno, nebo Å¾e dvÄ› slova spolu vÃ½znamovÄ› souvisejÃ­, i kdyÅ¾ jsou od sebe v textu daleko.
            </p>
            <p>
              DÅ¯leÅ¾itÃ© je, Å¾e attention v transformerech nefunguje jen jednou, ale ve vÃ­ce paralelnÃ­ch â€hlavÃ¡ch" (attention heads). KaÅ¾dÃ¡ z tÄ›chto hlav se mÅ¯Å¾e bÄ›hem trÃ©novÃ¡nÃ­ nauÄit sledovat jinÃ½ typ vztahÅ¯ v datech. NÄ›kterÃ© hlavy se mohou zamÄ›Å™ovat napÅ™Ã­klad na:
            </p>
            <ul>
              <li>syntaktickÃ© vztahy (kdo je podmÄ›t, co je pÅ™Ã­sudek),</li>
              <li>vÃ½znamovÃ© souvislosti (slova se stejnÃ½m nebo podobnÃ½m vÃ½znamem),</li>
              <li>odkazy a zÃ¡jmena (â€on", â€ona", â€to", k Äemu se vztahujÃ­),</li>
              <li>strukturu textu (co patÅ™Ã­ k sobÄ› v rÃ¡mci vÄ›ty, odstavce nebo dokumentu),</li>
              <li>nebo tÅ™eba Äasovou Äi logickou nÃ¡vaznost.</li>
            </ul>
            <p>
              DÃ­ky tomu si model sklÃ¡dÃ¡ vÃ½slednÃ© porozumÄ›nÃ­ textu z vÃ­ce pohledÅ¯ najednou, podobnÄ› jako kdyby se na stejnÃ½ problÃ©m dÃ­valo nÄ›kolik specialistÅ¯ souÄasnÄ›.
            </p>
            <p>
              V praxi to znamenÃ¡, Å¾e model dokÃ¡Å¾e zachytit vztahy napÅ™Ã­Ä celÃ½m vstupem. U textu napÅ™Ã­klad pochopÃ­, Å¾e pojem uvedenÃ½ na zaÄÃ¡tku odstavce je dÅ¯leÅ¾itÃ½ i pro jeho zÃ¡vÄ›r. U obrazovÃ½ch dat mÅ¯Å¾e attention zvÃ½raznit oblasti obrazu, kterÃ© jsou klÃ­ÄovÃ© pro rozpoznÃ¡nÃ­ objektu â€“ napÅ™Ã­klad konkrÃ©tnÃ­ tvar, strukturu nebo detail.
            </p>
            <p>
              Mechanismus attention pÅ™iÅ™azuje jednotlivÃ½m ÄÃ¡stem vstupu vÃ¡hy podle jejich relevance a z tÄ›chto vÃ¡Å¾enÃ½ch informacÃ­ vytvÃ¡Å™Ã­ vnitÅ™nÃ­ reprezentaci dat. BÄ›hem trÃ©novÃ¡nÃ­ se model uÄÃ­, kam mÃ¡ â€smÄ›Å™ovat pozornost", a tuto schopnost pak vyuÅ¾Ã­vÃ¡ pÅ™i generovÃ¡nÃ­ textu, shrnovÃ¡nÃ­ dokumentÅ¯ nebo predikci dalÅ¡Ã­ch krokÅ¯ â€“ bez nutnosti ruÄnÄ› definovanÃ½ch pravidel.
            </p>
            <p>
              Pro nÃ¡zornou interaktivnÃ­ vizualizaci a detailnÃ­ popis doporuÄujeme: <a href="https://poloclub.github.io/transformer-explainer/" target="_blank" rel="noopener noreferrer">Transformer Explainer</a>.
            </p>

          </div>
        </div>

        <!-- Jak se uÄÃ­ -->
        <section id="JakSeUci"> </section>
        <h1>Jak se LLM uÄÃ­</h1>
        <p>
          VelkÃ© jazykovÃ© modely (LLM) se uÄÃ­ z obrovskÃ½ch mnoÅ¾stvÃ­ textu â€“ knih, odbornÃ½ch ÄlÃ¡nkÅ¯, encyklopediÃ­, webovÃ½ch strÃ¡nek, technickÃ½ch dokumentacÃ­, veÅ™ejnÃ½ch diskuzÃ­ apod. NÄ›kdy takovÃ½ sbÄ›r dat narÃ¡Å¾Ã­ na otÃ¡zky autorskÃ½ch prÃ¡v a je zÅ™ejmÃ©, Å¾e ne ke vÅ¡em zdrojÅ¯m se firmy v minulosti dostaly zcela legÃ¡lnÄ›.
        </p>
        <p>
          RÅ¯znÃ­ poskytovatelÃ© modelÅ¯ (OpenAI, Anthropic, Google, Meta a dalÅ¡Ã­) pouÅ¾Ã­vajÃ­ odliÅ¡nÃ© kombinace dat a rÅ¯znÃ© metody jejich filtrovÃ¡nÃ­. SpoleÄnÃ½m cÃ­lem je ale vÅ¾dy totÃ©Å¾: zÃ­skat co nejÅ¡irÅ¡Ã­, jazykovÄ› bohatÃ½ a zÃ¡roveÅˆ dostateÄnÄ› kvalitnÃ­ vzorek textu, ze kterÃ©ho se model nauÄÃ­ obecnÃ© jazykovÃ© struktury i specializovanÄ›jÅ¡Ã­ styly psanÃ­. VelkÃ¡ ÄÃ¡st prÃ¡ce nespoÄÃ­vÃ¡ totiÅ¾ jen ve sbÄ›ru dat, ale prÃ¡vÄ› v jejich peÄlivÃ©m vÃ½bÄ›ru, ÄiÅ¡tÄ›nÃ­ a filtrovÃ¡nÃ­. Texty plnÃ© Å¡umu, ironie, trollingu nebo nekonzistentnÃ­ch informacÃ­ mohou model spÃ­Å¡e zmÃ¡st neÅ¾ nauÄit nÄ›co uÅ¾iteÄnÃ©ho.
        </p>
        <p>
          JinÃ½mi slovy: â€vezmeme celÃ½ internet a hotovo" nefunguje. Kdyby model trÃ©noval bez filtru tÅ™eba jen na nÃ¡hodnÃ©m vÃ½bÄ›ru z Redditu, nauÄil by se sice rychle reagovat, ale zÃ¡roveÅˆ by si osvojil Å™adu zvlÃ¡Å¡tnÃ­ch nÃ¡vykÅ¯, kterÃ© u asistenta vÄ›tÅ¡inou nechceme.
        </p>
        <p>
          Je takÃ© velmi dÅ¯leÅ¾itÃ© mÃ­t na pamÄ›ti, Å¾e jazykovÃ½ model:
        </p>
        <ul>
          <li>pÅ™i odpovÃ­dÃ¡nÃ­ neÄerpÃ¡ z pÅ¯vodnÃ­ databÃ¡ze zdrojÅ¯,</li>
          <li>nepamatuje si konkrÃ©tnÃ­ dokumenty pouÅ¾itÃ© pÅ™i trÃ©ninku,</li>
          <li>aktivnÄ› nevyhledÃ¡vÃ¡ informace, pokud k tomu nenÃ­ vÃ½slovnÄ› pÅ™ipojen (napÅ™Ã­klad pÅ™es vyhledÃ¡vaÄ).</li>
        </ul>
        <p>
          ZjednoduÅ¡enÄ› Å™eÄeno: model si trÃ©novacÃ­ data uloÅ¾Ã­ v silnÄ› komprimovanÃ© podobÄ›. NÄ›kterÃ¡ fakta znÃ¡ lÃ©pe (ta, kterÃ¡ se v datech objevovala Äasto), jinÃ¡ si nezapamatuje vÅ¯bec. PÅ™ipomÃ­nÃ¡ to studenta, kterÃ½ se noc pÅ™ed zkouÅ¡kou nadrtil skripta a nÄ›co si pamatuje velmi pÅ™esnÄ›, nÄ›co mlhavÄ› a nÄ›co vÅ¯bec.
        </p>

        <!-- PrvnÃ­ fÃ¡ze -->
        <section id="ZakladniTrenink"> </section>
        <h2><b>PrvnÃ­ fÃ¡ze:</b> ZÃ¡kladnÃ­ jazykovÃ½ trÃ©nink</h2>
        <p>V prvnÃ­ fÃ¡zi je model pÅ™edtrÃ©novÃ¡n na obrovskÃ©m mnoÅ¾stvÃ­ textÅ¯. BÄ›hem tohoto trÃ©ninku:</p>
        <ul>
          <li>dostÃ¡vÃ¡ text rozdÄ›lenÃ½ na malÃ© jednotky zvanÃ© 
            <span class="info" data-info>
              <span class="info__trigger info__word" tabindex="0" role="button" aria-label="VÃ­ce informacÃ­ o tokenech">
                tokeny
              </span>
              <span class="info__tooltip" role="tooltip" hidden>
                Token nenÃ­ vÅ¾dy celÃ© slovo â€“ mÅ¯Å¾e to bÃ½t i ÄÃ¡st slova, interpunkÄnÃ­ znamÃ©nko nebo speciÃ¡lnÃ­ znak. NapÅ™Ã­klad v angliÄtinÄ› je â€international" 
                    Äasto rozdÄ›leno na inter + national, zatÃ­mco bÄ›Å¾nÃ¡ slova jako â€cat" bÃ½vajÃ­ jeden token.
              </span>
            </span>,
          </li>
          <li>uÄÃ­ se pÅ™edpovÃ­dat nÃ¡sledujÃ­cÃ­ token v sekvenci,</li>
          <li>postupnou optimalizacÃ­ milionÅ¯ aÅ¾ miliard parametrÅ¯ si vytvÃ¡Å™Ã­ vnitÅ™nÃ­ reprezentaci jazykovÃ½ch vzorÅ¯.</li>
        </ul>
        <p>
          VÃ½sledkem je neuronovÃ¡ sÃ­Å¥ s natrÃ©novanÃ½mi parametry (vÄetnÄ› attention mechanismÅ¯), kterÃ¡ velmi dobÅ™e â€umÃ­ jazyk". NeuÄÃ­ se ale fakta jako poloÅ¾ky v databÃ¡zi. MÃ­sto toho se uÄÃ­ statistickÃ© zÃ¡vislosti jazyka. VÃ­, co obvykle nÃ¡sleduje po Äem, co spolu souvisÃ­ a co znÃ­ pravdÄ›podobnÄ›.
        </p>
        <p>
          Po tÃ©to fÃ¡zi mÃ¡me hotovÃ©ho jazykovÃ©ho specialistuâ€“asociÃ¡la: perfektnÄ› znÃ¡ jazyky, ale netuÅ¡Ã­, jak komunikovat s lidmi. UmÃ­ generovat text, ale nevÃ­, Å¾e mÃ¡ bÃ½t uÅ¾iteÄnÃ½, sluÅ¡nÃ½ nebo Å¾e by mÄ›l odpovÃ­dat na otÃ¡zky. To pÅ™ijde aÅ¾ v dalÅ¡Ã­ fÃ¡zi vÃ½cviku.
        </p>

        <!-- DruhÃ¡ fÃ¡ze -->
        <section id="Finetuning"> </section> 
        <h2><b>DruhÃ¡ fÃ¡ze:</b> DoladÄ›nÃ­ a interakce</h2>
        <p>
          Aby byl model prakticky pouÅ¾itelnÃ½, nÃ¡sleduje fÃ¡ze doladÄ›nÃ­ (fine-tuning). V tÃ©to fÃ¡zi se model uÄÃ­ reagovat na instrukce a komunikovat s ÄlovÄ›kem. PouÅ¾Ã­vajÃ­ se napÅ™Ã­klad:
        </p>
        <ul>
          <li>vzorovÃ© otÃ¡zky a odpovÄ›di,</li>
          <li>pÅ™edepsanÃ© dialogy,</li>
          <li>instrukce typu â€shrÅˆ", â€vysvÄ›tli", â€porovnej", â€navrhni".</li>
        </ul>
        <p>
          Model se tak neuÄÃ­ jen co Å™Ã­kat, ale i jak to Å™Ã­kat. Tato fÃ¡ze je klÃ­ÄovÃ¡ pro to, aby LLM nepÅ¯sobil jako nÃ¡hodnÃ½ generÃ¡tor textu, ale jako asistent, kterÃ½ smysluplnÄ› reaguje na zadÃ¡nÃ­.
        </p>

        <!-- TÅ™etÃ­ fÃ¡ze -->
        <section id="Reinforcement"> </section> 
        <h2>Reinforcement learning: vÃ½cvik na lidi</h2>
        <p>
          NÄ›kterÃ© modely prochÃ¡zejÃ­ jeÅ¡tÄ› dalÅ¡Ã­ fÃ¡zÃ­ zvanou <b>reinforcement learning from human feedback (RLHF)</b> â€“ tedy uÄenÃ­m z lidskÃ© zpÄ›tnÃ© vazby. Tato fÃ¡ze uÅ¾ nepÅ™idÃ¡vÃ¡ novÃ© znalosti o svÄ›tÄ›, ale uÄÃ­ model jak se chovat, aby jeho odpovÄ›di byly pro lidi uÅ¾iteÄnÃ© a pÅ™ijatelnÃ©.
        </p>
        <p>Proces probÃ­hÃ¡ zhruba takto:</p>
        <ul>
          <li>model vygeneruje nÄ›kolik moÅ¾nÃ½ch odpovÄ›dÃ­ na stejnÃ½ dotaz,</li>
          <li>lidÃ© (hodnotitelÃ©) tyto odpovÄ›di porovnajÃ­ a oznaÄÃ­ ty, kterÃ© jsou uÅ¾iteÄnÄ›jÅ¡Ã­, pÅ™esnÄ›jÅ¡Ã­ nebo stylisticky vhodnÄ›jÅ¡Ã­,</li>
          <li>model se postupnÄ› uÄÃ­ preferovat odpovÄ›di, kterÃ© lidÃ© hodnotÃ­ lÃ©pe.</li>
        </ul>
        <p>
          CelÃ½ proces lze velmi dobÅ™e pÅ™irovnat ke kynologickÃ©mu vÃ½cviku. Model zkouÅ¡Ã­ rÅ¯znÃ© â€triky" a podle reakce lidÃ­ se uÄÃ­, kterÃ© z nich mÃ¡ opakovat ÄastÄ›ji. A stejnÄ› jako u vÃ½cviku psa platÃ­, Å¾e zÃ¡leÅ¾Ã­ na tom, kdo drÅ¾Ã­ pamlsky. Model se neuÄÃ­ â€sprÃ¡vnÃ© odpovÄ›di", ale ty, kterÃ© konkrÃ©tnÃ­ skupina lidÃ­ povaÅ¾uje za Å¾Ã¡doucÃ­.
        </p>
        <p>
          VÃ½sledkem je model, kterÃ½ se nauÄil pÅ™edevÅ¡Ã­m to, jak odpovÃ­dat tak, aby to lidem vyhovovalo.
        </p>

        <!-- UmÄ›nÃ­ dialogu -->
        <h2>UmÄ›nÃ­ dialogu</h2>
        <p>
          Na zÃ¡kladÄ› vzorovÃ½ch konverzacÃ­ se model uÄÃ­ vÃ©st plynulÃ½ dialog. RozliÅ¡uje roli uÅ¾ivatele a asistenta, uÄÃ­ se navazovat na pÅ™edchozÃ­ odpovÄ›di, reagovat na doplÅˆujÃ­cÃ­ otÃ¡zky a udrÅ¾ovat kontext v prÅ¯bÄ›hu konverzace.
        </p>
        <p>
          PodobnÄ› jako student u ÃºstnÃ­ zkouÅ¡ky jsou konverzaÄnÃ­ LLM vycviÄeny tak, aby vÅ¾dy nÄ›jak odpovÄ›dÄ›ly â€“ i kdyÅ¾ si nejsou ÃºplnÄ› jistÃ©. To je v praxi Äasto uÅ¾iteÄnÃ©, ale zÃ¡roveÅˆ to vede k jevu zvanÃ©mu <b>halucinace</b>: model odpovÃ­dÃ¡ sebevÄ›domÄ› i tam, kde ve skuteÄnosti Å¾Ã¡dnou spolehlivou informaci nemÃ¡.
        </p>

        <h2>Co se model neuÄÃ­</h2>
        <p>
          I kdyÅ¾ jazykovÃ© modely Äasto pÅ¯sobÃ­ velmi sebejistÄ›, je dobrÃ© mÃ­t na pamÄ›ti, Å¾e se:
        </p>
        <ul>
          <li>neuÄÃ­ logiku ve smyslu formÃ¡lnÃ­ch pravidel nebo dÅ¯kazÅ¯,</li>
          <li>neuÄÃ­ metodologii vÃ½zkumu,</li>
          <li>neuÄÃ­ se fakta jako poloÅ¾ky v databÃ¡zi,</li>
          <li>neuÄÃ­ se ovÄ›Å™ovat pravdivost informacÃ­,</li>
          <li>nerozliÅ¡ujÃ­ pravdu a nepravdu jako takovou.</li>
        </ul>
        <p>
          Model v zÃ¡sadÄ› pouze odhaduje, co by mÄ›lo pravdÄ›podobnÄ› nÃ¡sledovat â€“ nikoli to, co je skuteÄnÄ› pravda. To, Å¾e odpovÄ›Ä znÃ­ rozumnÄ› a plynule, tedy jeÅ¡tÄ› neznamenÃ¡, Å¾e je sprÃ¡vnÃ¡.
        </p>

        <!-- Jak chÃ¡pe text -->
        <section id="JakChapeText"> </section>
        <h1>Jak LLM â€chÃ¡pe" text</h1>
        <p>
          KdyÅ¾ ÄlovÄ›k Äte text, pracuje s vÃ½znamem. ChÃ¡pe slova, vÄ›ty, souvislosti a Äte mezi Å™Ã¡dky. JazykovÃ½ model tohle neumÃ­ a proto si musÃ­ text nejdÅ™Ã­v pÅ™edÅ¾vÃ½kat do podoby, kterÃ© jeho kÅ™emÃ­kovÃ½ mozek rozumÃ­. Jinak by si nepÅ™eÄetl ani BabiÄku.
        </p>
        <p>
          PrvnÃ­m krokem je tokenizace. Text se rozdÄ›lÃ­ na malÃ© kousky â€“ takzvanÃ© <a href="#Tokeny">tokeny</a>. Ty Äasto neodpovÃ­dajÃ­ celÃ½m slovÅ¯m, ale mÅ¯Å¾e jÃ­t o ÄÃ¡st slova, interpunkci, mezeru nebo jejich kombinaci. KaÅ¾dÃ©mu tokenu odpovÃ­dÃ¡ jednoznaÄnÃ¡ ÄÃ­selnÃ¡ hodnota, takÅ¾e vÃ½sledkem uÅ¾ nenÃ­ text, ale posloupnost ÄÃ­sel. A prÃ¡vÄ› s touhle ÄÃ­selnou verzÃ­ jazyka model dÃ¡l pracuje.
        </p>

        <h3>Jak to vidÃ­ model: vyzkouÅ¡ejte si tokenizaci</h3>
        <p>
          NevÄ›Å™Ã­te, Å¾e model â€nevidÃ­" slova, ale jen tokeny? Zkuste si to sami. Zadejte libovolnou vÄ›tu a podÃ­vejte se, na jakÃ© tokeny ji model rozdÄ›lÃ­. ÄŒasto to nejsou kousky, kterÃ© by ÄlovÄ›ka napadly na prvnÃ­ dobrou.
        </p>

        <!-- Tokenizer Demo -->
        <div id="tokenizer-demo" class="tokenizer-demo">
          <textarea class="tokenizer-demo__input" placeholder="NapiÅ¡te nebo vloÅ¾te text...">â€Dejte na vÅ¡ecko pozor, ohlÃ­dnÄ›te se po drÅ¯beÅ¾i!" pÅ™ikazuje babiÄka. SultÃ¡n chce Adelce se lichotit, ÄuchÃ¡ k vÄ›ncÅ¯m, kterÃ© nese v ruce, ale ona zdvihÃ¡ obÄ› ruce vzhÅ¯ru a babiÄka odhÃ¡nÃ­ ho Å™kouc: â€NevidÃ­Å¡, ty hloupÃ½, Å¾e je Adelka druÅ¾iÄkou?"</textarea>
          
          <div class="tokenizer-demo__results">
            <div class="tokenizer-demo__stats">
              <div class="tokenizer-demo__stat">
                <span class="tokenizer-demo__stat-value">0</span>
                <span class="tokenizer-demo__stat-label">tokenÅ¯</span>
              </div>
              <div class="tokenizer-demo__stat">
                <span class="tokenizer-demo__stat-value">0</span>
                <span class="tokenizer-demo__stat-label">znakÅ¯</span>
              </div>
              <div class="tokenizer-demo__stat">
                <span class="tokenizer-demo__stat-value">0</span>
                <span class="tokenizer-demo__stat-label">tokenu na znak</span>
              </div>
            </div>
            <div class="tokenizer-demo__visual"></div>
            <div class="tokenizer-demo__tokens"></div>
            <p class="tokenizer-demo__note">PoznÃ¡mka: KaÅ¾dÃ½ model mÃ¡ vlastnÃ­ tokenizÃ©r. StejnÃ½ text tak mÅ¯Å¾e bÃ½t rÅ¯znÃ½mi modely rozdÄ›len na odliÅ¡nÃ½ poÄet tokenÅ¯.</p>
          </div>
        </div>

        <p>
          SamotnÃ¡ ÄÃ­sla by ale byla dost neuÅ¾iteÄnÃ¡, takÅ¾e pÅ™ichÃ¡zÃ­ dalÅ¡Ã­ krok: <a href="#Embeddingy">embeddingy</a>. KaÅ¾dÃ©mu tokenu je pÅ™iÅ™azena souÅ™adnice ve vektorovÃ©m prostoru o velmi vysokÃ© dimenzi (Å™Ã¡dovÄ› tisÃ­ce rozmÄ›rÅ¯). Tento vektor funguje jako ÄÃ­selnÃ½ otisk vÃ½znamu a kontextu.
        </p>
        <p>
          ExistujÃ­ i specializovanÃ© embeddingovÃ© modely, jejichÅ¾ jedinÃ½m Ãºkolem je pÅ™evÃ¡dÄ›t text do tohoto vektorovÃ©ho prostoru. AÅ¥ uÅ¾ jde o samostatnÃ½ model nebo souÄÃ¡st LLM, vÃ½sledkem je prostor, ve kterÃ©m jsou vektory uspoÅ™Ã¡dÃ¡ny tak, Å¾e vÃ½znamovÄ› podobnÃ© tokeny leÅ¾Ã­ blÃ­zko sebe, zatÃ­mco nesouvisejÃ­cÃ­ pojmy jsou daleko.
        </p>
        <p>
          V tomto vektorovÃ©m prostoru pak zaÄnou platit zajÃ­mavÃ© vÄ›ci:
        </p>
        <ul>
          <li>tokeny s podobnÃ½m vÃ½znamem leÅ¾Ã­ blÃ­zko sebe,</li>
          <li>rozdÃ­ly mezi vektory mohou reprezentovat vztahy,</li>
          <li>jazyk se mÄ›nÃ­ v geometrii.</li>
        </ul>

        <p>
          Pokud mÃ¡me k dispozici vektory, mÅ¯Å¾eme s nimi pracovat podobnÄ› jako v bÄ›Å¾nÃ© geometrii â€“ sÄÃ­tat je, odeÄÃ­tat a porovnÃ¡vat jejich smÄ›r a vzdÃ¡lenost. PrÃ¡vÄ› dÃ­ky tomu se v embeddingovÃ©m prostoru objevujÃ­ konzistentnÃ­ vÃ½znamovÃ© vztahy. KlasickÃ½ (a dnes uÅ¾ legendÃ¡rnÃ­) pÅ™Ã­klad je:
        </p>
        <p style="text-align: center; font-size: 1.1em; font-weight: bold;">
          vektor(krÃ¡l) âˆ’ vektor(muÅ¾) + vektor(Å¾ena) â‰ˆ vektor(krÃ¡lovna)
        </p>
        <p>
          Model samozÅ™ejmÄ› netuÅ¡Ã­, co je monarchie nebo pohlavÃ­. Jen se z dat nauÄil, Å¾e urÄitÃ½ vÃ½znamovÃ½ rozdÃ­l se v tomto prostoru chovÃ¡ jako konzistentnÃ­ smÄ›r. PodobnÃ© â€smÄ›ry" odpovÃ­dajÃ­ rÅ¯znÃ½m aspektÅ¯m vÃ½znamu â€“ napÅ™Ã­klad roli, pohlavÃ­, ÄasovÃ©mu zasazenÃ­ nebo mÃ­Å™e abstrakce.
        </p>

        <!-- VzdÃ¡lenosti slov v prostoru - Notecard-->
        <div class="section section--note">
          <h3 class="section__title section__title--note">VzdÃ¡lenosti slov v prostoru</h3>

          <p>PodobnÄ› jsou si blÃ­zkÃ© pojmy jako:</p>
          <ul>
            <li>krÃ¡l â€” krÃ¡lovna,</li>
            <li>auto â€” vozidlo,</li>
            <li>uÄitel â€” Å¡kola,</li>
            <li>lÃ©kaÅ™ â€” nemocnice.</li>
          </ul>
          <p>A naopak velmi vzdÃ¡lenÃ© jsou tÅ™eba:</p>
          <ul>
            <li>koÄka â€” kvark,</li>
            <li>matematika â€” banÃ¡n.</li>
          </ul>
        </div>

        <p>
          DÅ¯leÅ¾itÃ© je, Å¾e u modernÃ­ch LLM nenÃ­ vÃ½znam slova pevnÄ› danÃ½. StejnÃ© slovo mÃ¡ jinou reprezentaci podle kontextu â€“ â€klÃ­Ä" ve vÄ›tÄ› â€ztratil jsem klÃ­Ä od dveÅ™Ã­" a â€houslovÃ½ klÃ­Ä" skonÄÃ­ na ÃºplnÄ› jinÃ½ch mÃ­stech v prostoru.
        </p>

        <h3>ChÃ¡pe to tedy model jako ÄlovÄ›k?</h3>
        <p>
          Ne. LLM nepracuje s vÃ½znamem ani zÃ¡mÄ›rem. NemÃ¡ plÃ¡n, nevÃ­, â€o Äem mluvÃ­", a nic si nepÅ™edstavuje. DÄ›lÃ¡ jednu jedinou vÄ›c: predikuje dalÅ¡Ã­ token, kterÃ½ je v danÃ©m kontextu nejpravdÄ›podobnÄ›jÅ¡Ã­.
        </p>
        <p>
          ZnÃ­ to banÃ¡lnÄ› â€“ skoro jako chytrÃ½ naÅ¡eptÃ¡vaÄ. JenÅ¾e kombinace:
        </p>
        <ul>
          <li>obrovskÃ©ho mnoÅ¾stvÃ­ dat,</li>
          <li>vysokÃ© dimenzionality embeddingÅ¯,</li>
          <li>a sofistikovanÃ© architektury (transformery, attention)</li>
        </ul>
        <p>
          vede k chovÃ¡nÃ­, kterÃ© velmi pÅ™esvÄ›dÄivÄ› pÅ™ipomÃ­nÃ¡ porozumÄ›nÃ­. A pÅ™ekvapivÄ› Äasto to staÄÃ­.
        </p>

        <!-- HistorickÃ© okÃ©nko: word2vec -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-word2vec" id="collapsible-button-word2vec">
          <b>VÃCE:</b> HistorickÃ© okÃ©nko: word2vec a TomÃ¡Å¡ Mikolov
          </button>

          <div class="collapsible__panel" id="collapsible-panel-word2vec" role="region" aria-labelledby="collapsible-button-word2vec" hidden>
            <p>
              MyÅ¡lenka reprezentovat slova jako body ve vÃ­cerozmÄ›rnÃ©m prostoru se vÃ½raznÄ› prosadila kolem roku 2013 dÃ­ky modelu <b>word2vec</b>, jehoÅ¾ hlavnÃ­m autorem byl TomÃ¡Å¡ Mikolov (tehdy Google Research). Word2vec ukÃ¡zal, Å¾e pouhÃ½m sledovÃ¡nÃ­m kontextu slov ve velkÃ½ch textech lze zachytit vÃ½znamovÃ© vztahy â€“ bez ruÄnÄ› definovanÃ½ch pravidel nebo slovnÃ­kÅ¯.
            </p>
            <p>
              TehdejÅ¡Ã­ modely pracovaly se statickÃ½mi embeddingy, kde mÄ›lo kaÅ¾dÃ© slovo jednu pevnou reprezentaci. ModernÃ­ jazykovÃ© modely tuto myÅ¡lenku rozÅ¡iÅ™ujÃ­ o kontextovÃ© embeddingy, kde se vÃ½znam slova mÄ›nÃ­ podle okolnÃ­ho textu. ZÃ¡kladnÃ­ princip ale zÅ¯stÃ¡vÃ¡ stejnÃ½: vÃ½znam vznikÃ¡ z dat a projevuje se geometricky.
            </p>
          </div>
        </div>

        <!-- Jak LLM vybÃ­rÃ¡ dalÅ¡Ã­ slovo -->
        <section id="JakVybiraSlovo"> </section>
        <h1>Jak LLM vybÃ­rÃ¡ dalÅ¡Ã­ slovo</h1>
        <p>
          KdyÅ¾ jazykovÃ½ model generuje text, nehledÃ¡ jedno jedinÃ© â€sprÃ¡vnÃ©" slovo, kterÃ© by mÄ›lo nÃ¡sledovat. V kaÅ¾dÃ©m kroku nejprve proÅ¾ene vstup pÅ™es attention mechanismy a neuronovou sÃ­Å¥ a na jejich zÃ¡kladÄ› vytvoÅ™Ã­ pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­ nad vÅ¡emi tokeny, kterÃ© mÃ¡ ve svÃ©m slovnÃ­ku.
        </p>
        <p>
          VÃ½sledkem tedy nenÃ­ odpovÄ›Ä typu â€tady je dalÅ¡Ã­ slovo", ale spÃ­Å¡ nÄ›co jako:
        </p>
        <p style="margin-left: 2em; font-style: italic;">
          â€TÄ›chto deset tokenÅ¯ dÃ¡vÃ¡ smysl, tyhle tÅ™i jsou hodnÄ› pravdÄ›podobnÃ©, pÃ¡r dalÅ¡Ã­ch by se jeÅ¡tÄ› dalo zkousnout a zbytek je spÃ­Å¡ nesmysl."
        </p>
        <p>
          Teprve z tohoto rozdÄ›lenÃ­ se jeden token vybere a prÃ¡vÄ› zpÅ¯sob, jakÃ½m se vybÃ­rÃ¡, mÃ¡ zÃ¡sadnÃ­ vliv na to, jak budou odpovÄ›di modelu pÅ¯sobit.
        </p>

        <h3>Nejde o vÃ½bÄ›r, ale o losovÃ¡nÃ­</h3>
        <p>
          ÄŒasto se Å™Ã­kÃ¡, Å¾e LLM produkujÃ­ â€nejpravdÄ›podobnÄ›jÅ¡Ã­" pokraÄovÃ¡nÃ­ textu. To ale nenÃ­ ÃºplnÄ› pÅ™esnÃ©. JazykovÃ½ model si v kaÅ¾dÃ©m kroku vytvoÅ™Ã­ pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­ moÅ¾nÃ½ch pokraÄovÃ¡nÃ­ a z nÄ›j losuje dalÅ¡Ã­ token. Tokeny s vyÅ¡Å¡Ã­ pravdÄ›podobnostÃ­ majÃ­ vÄ›tÅ¡Ã­ Å¡anci, Å¾e budou vybrÃ¡ny, ale nenÃ­ to Å¾Ã¡dnÃ¡ jistota.
        </p>
        <p>
          DÃ­ky tomu:
        </p>
        <ul>
          <li>odpovÄ›di nejsou pokaÅ¾dÃ© ÃºplnÄ› stejnÃ©,</li>
          <li>model mÅ¯Å¾e bÃ½t kreativnÃ­,</li>
          <li>ale zÃ¡roveÅˆ se mÅ¯Å¾e i â€utrhnout ze Å™etÄ›zu".</li>
        </ul>

        <!-- ObrÃ¡zek: NastavenÃ­ teploty v Google AI Studio -->
        <figure class="image-card image-card--thumbnail">
          <div class="image-card__frame">
            <div class="thumbnail" data-full="img/teplota.png">
              <img src="img/teplota.png" alt="NastavenÃ­ Temperature a Top P v Google AI Studio" loading="lazy" decoding="async"/>
            </div>
          </div>
          <figcaption class="image-card__caption">
            <h3 class="image-card__title">NastavenÃ­ Temperature a Top P</h3>
            <p class="image-card__meta">
              KliknÄ›te pro zvÄ›tÅ¡enÃ­.
            </p>
          </figcaption>
        </figure>

        <p>
          V nÄ›kterÃ½ch LLM nÃ¡strojÃ­ch mÃ¡ uÅ¾ivatel moÅ¾nost pravidla tÃ©to ruskÃ© rulety ovlivnit explicitnÃ­m nastavenÃ­m parametrÅ¯ jako je teplota, top-k nebo top-p.
        </p>

        <h3>Teplota: jak moc se mÅ¯Å¾e model odvÃ¡zat</h3>
        <p>
          <a href="#Temperature">Teplota</a> (temperature) urÄuje, jak moc se model pÅ™i losovÃ¡nÃ­ drÅ¾Ã­ tÄ›ch nejpravdÄ›podobnÄ›jÅ¡Ã­ch moÅ¾nostÃ­.
        </p>
        <ul>
          <li><strong>NÃ­zkÃ¡ teplota â†’ model hraje na jistotu</strong><br>
          VybÃ­rÃ¡ hlavnÄ› nejpravdÄ›podobnÄ›jÅ¡Ã­ tokeny. VÃ½stupy jsou konzistentnÃ­, opatrnÃ© a Äasto trochuâ€¦ nudnÃ©.</li>
          <li><strong>VyÅ¡Å¡Ã­ teplota â†’ model vÃ­c experimentuje</strong><br>
          Do hry se dostÃ¡vajÃ­ i mÃ©nÄ› pravdÄ›podobnÃ© tokeny. Text mÅ¯Å¾e bÃ½t kreativnÄ›jÅ¡Ã­, originÃ¡lnÄ›jÅ¡Ã­, ale Äasto taky ÃºplnÄ› mimo.</li>
        </ul>

        <!-- Info bublina: ProÄ se tomu Å™Ã­kÃ¡ teplota -->
        <div class="collapsible" data-collapsible>
          <button class="collapsible__toggle" aria-expanded="false" aria-controls="collapsible-panel-teplota" id="collapsible-button-teplota">
          <b>VÃCE:</b> ProÄ se tomu Å™Ã­kÃ¡ â€teplota" (a kdo se tu vlastnÄ› zahÅ™Ã­vÃ¡)
          </button>

          <div class="collapsible__panel" id="collapsible-panel-teplota" role="region" aria-labelledby="collapsible-button-teplota" hidden>
            <p>
              PÅ™i generovÃ¡nÃ­ textu jazykovÃ½ model nejprve spoÄÃ­tÃ¡ pro kaÅ¾dÃ½ moÅ¾nÃ½ dalÅ¡Ã­ token skÃ³re (tzv. logit), kterÃ© vyjadÅ™uje, jak dobÅ™e danÃ½ token zapadÃ¡ do kontextu. Tato skÃ³re se nÃ¡slednÄ› pÅ™evÃ¡dÄ›jÃ­ na pravdÄ›podobnosti pomocÃ­ funkce zvanÃ© <em>softmax</em>:
            </p>
            <p style="text-align: center; font-size: 1.2em;">
              $$P(\text{token}_i) = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$$
            </p>
            <p>
              kde \(z_i\) je skÃ³re (logit) tokenu, \(T\) je teplota a jmenovatel zajiÅ¡Å¥uje, Å¾e souÄet vÅ¡ech pravdÄ›podobnostÃ­ je roven 1. Softmax se aplikuje na konci kaÅ¾dÃ©ho generaÄnÃ­ho kroku. NeuronovÃ¡ sÃ­Å¥ nejprve vypoÄÃ­tÃ¡ surovÃ¡ skÃ³re pro vÅ¡echny tokeny ve slovnÃ­ku a softmax je pÅ™evede na pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­, ze kterÃ©ho se nÃ¡slednÄ› jeden token vybere (vzorkovÃ¡nÃ­m). PrÃ¡vÄ› v tomto kroku se uplatÅˆuje teplota i dalÅ¡Ã­ parametry generovÃ¡nÃ­.
            </p>
            <p>
              Tvar tÃ©to rovnice nenÃ­ nÃ¡hodnÃ½ â€“ exponenciÃ¡lnÃ­ Älen je stejnÃ½ jako v <em>BoltzmannovÄ› rozdÄ›lenÃ­</em> ze statistickÃ© fyziky. Tam teplota urÄuje, jak snadno systÃ©m pÅ™echÃ¡zÃ­ do mÃ©nÄ› pravdÄ›podobnÃ½ch (vyÅ¡Å¡Ã­ch energetickÃ½ch) stavÅ¯. U jazykovÃ½ch modelÅ¯ hraje teplota stejnou roli: urÄuje, jak snadno model sÃ¡hne po mÃ©nÄ› pravdÄ›podobnÃ©m tokenu.
            </p>
            <p>
              <strong>Co se stane pÅ™i velmi nÃ­zkÃ© teplotÄ›?</strong><br>
              PÅ™i \(T \to 0\) se rozdÃ­ly mezi skÃ³re v exponenciÃ¡le extrÃ©mnÄ› zesÃ­lÃ­, takÅ¾e tÃ©mÄ›Å™ veÅ¡kerÃ¡ pravdÄ›podobnost â€spadne" na jedinÃ½ nejlÃ©pe hodnocenÃ½ token. LosovÃ¡nÃ­ se tÃ­m pÃ¡dem prakticky zmÄ›nÃ­ v deterministickÃ½ vÃ½bÄ›r a model pokaÅ¾dÃ© Å™ekne to samÃ©.
            </p>
            <p>
              <strong>StruÄnÄ›:</strong>
            </p>
            <ul>
              <li>nÃ­zkÃ¡ teplota â†’ jistota, opakovatelnost, minimum pÅ™ekvapenÃ­,</li>
              <li>vyÅ¡Å¡Ã­ teplota â†’ rozmanitost, kreativita, ale i vÄ›tÅ¡Ã­ riziko nesmyslÅ¯.</li>
            </ul>
            <p>
              <em>JazykovÃ½ model se pÅ™i vyÅ¡Å¡Ã­ teplotÄ› skuteÄnÄ› nezahÅ™Ã­vÃ¡. To, co se zahÅ™Ã­vÃ¡, je datacentrum, ve kterÃ©m bÄ›Å¾Ã­... a nÄ›kdy docela dost.</em> ğŸ”¥
            </p>
          </div>
        </div>

        <h3>Top-k a top-p: z jak velkÃ©ho klobouku losujeme</h3>
        <p>
          DalÅ¡Ã­ parametry Å™Ã­kajÃ­, z kolika moÅ¾nostÃ­ vÅ¯bec smÃ­ model vybÃ­rat.
        </p>
        <ul>
          <li><strong>Top-k:</strong> model si vezme k nejpravdÄ›podobnÄ›jÅ¡Ã­ch tokenÅ¯ a losuje jen z nich.<br>
          <span style="color: var(--clr-text-muted, #6b7280); font-size: 0.9em;">(NapÅ™. â€vybÃ­rej jen z 50 nejlepÅ¡Ã­ch nÃ¡vrhÅ¯.")</span></li>
          <li><strong>Top-p (nucleus sampling):</strong> model vybÃ­rÃ¡ z takovÃ©ho poÄtu tokenÅ¯, aby jejich souÄet pravdÄ›podobnostÃ­ dosÃ¡hl urÄitÃ© hranice (napÅ™. 90 %).</li>
        </ul>
        <p>
          Oba pÅ™Ã­stupy majÃ­ stejnÃ½ cÃ­l: odÅ™Ã­znout extrÃ©mnÄ› nepravdÄ›podobnÃ© nesmysly, aniÅ¾ by model musel jet ÃºplnÄ› â€na kolejÃ­ch".
        </p>
        <p>
          KromÄ› toho lze podobnÃ½ efekt Äasto dosÃ¡hnout jiÅ¾ instrukcÃ­ v promptu â€“ napÅ™Ã­klad tÃ­m, Å¾e v zadÃ¡nÃ­ Å™eknete modelu â€buÄ co nejvÃ­c kreativnÃ­" nebo â€drÅ¾ se pÅ™esnÄ› zadÃ¡nÃ­ a odpovÃ­dej podle tÃ©to Å¡ablony". Modely, kterÃ© jsou trÃ©novÃ¡ny nÃ¡sledovat pokyny, obvykle takto formulovanÃ© preference respektujÃ­ a chovÃ¡nÃ­ podle nich pÅ™izpÅ¯sobÃ­.
        </p>
        <p>
          <strong>Model sÃ¡m o sobÄ› nenÃ­ kreativnÃ­ ani opatrnÃ½. To, jak se chovÃ¡, je pÅ™Ã­mÃ½ dÅ¯sledek toho, jak mu nastavÃ­me pravidla losovÃ¡nÃ­.</strong>
        </p>

    </div>

  </div>
</section>
